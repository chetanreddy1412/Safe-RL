{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safety_gymnasium.wrappers.gymnasium_conversion import make_gymnasium_environment\n",
    "import gymnasium as gym\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import safety_gymnasium\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "from pynput.keyboard import Key, Listener\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO   \n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import Actor,Critic\n",
    "from torch.autograd import Variable\n",
    "from utils import Memory,OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories_df():\n",
    "    '''\n",
    "    Returns an Empty Trajectories Dataframe with the required columns for PointCircle Environment\n",
    "    It is used to collect the expert safe trajectories\n",
    "    '''\n",
    "    \n",
    "    sensor_names = ['accelerometer','velocimeter','gyro','magnetometer','circle_lidar'] #Having a seperate list to maintain the order of names\n",
    "    obs_config = {'accelerometer':3,'velocimeter':3,'gyro':3,'magnetometer':3,'circle_lidar':16}\n",
    "    columns_obs = []\n",
    "    for sensor_name in sensor_names:\n",
    "        num_obs_vals = obs_config[sensor_name]\n",
    "        columns_obs.extend(['obs_'+sensor_name+'_'+str(i) for i in range(num_obs_vals)])\n",
    "\n",
    "    columns_actions = ['act_force_linear','act_angular_velocity']\n",
    "\n",
    "    traj_columns_list = ['env_code','Episode','Time_Step'] + columns_obs + ['x','y','theta']+ columns_actions + ['reward','cost']\n",
    "    # columns_obs = ['accelerometer_{i}' for i in range(3)] + ['velocimeter_{i}' for i in range(3)] \n",
    "    trajectories_df = pd.DataFrame(columns=traj_columns_list)\n",
    "    return trajectories_df\n",
    "\n",
    "def scale_angle(angle):\n",
    "    '''Scales the Angles between 0 and 360, Input must be in degrees'''\n",
    "    \n",
    "    angle_scaled = angle%360\n",
    "\n",
    "    if angle_scaled<0:\n",
    "        angle_scaled+=360\n",
    "    \n",
    "    return angle_scaled\n",
    "\n",
    "def get_x_vals(x_indices,lidar_resolution):\n",
    "    '''Returns in Degrees'''\n",
    "\n",
    "    index_to_angle_factor = 360/lidar_resolution\n",
    "    x_vals = x_indices*index_to_angle_factor\n",
    "\n",
    "    # Handling the Case where there is a jump\n",
    "    # if len(x_indices)==0:\n",
    "    #     print(x_indices)\n",
    "    if max(x_indices)-min(x_indices)==2:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        if 1 in x_indices:\n",
    "            x_vals[0:2] += 360\n",
    "        else:\n",
    "            x_vals[0] += 360\n",
    "\n",
    "    return x_vals\n",
    "\n",
    "def get_lidar_r_theta(lidar_values,lidar_resolution,max_lidar_distance):\n",
    "    '''\n",
    "    The lidar values are processed and location of the center of the circle is returned from agent's frame of reference\n",
    "    \n",
    "    Returns\n",
    "        theta_lidar_rad, r: Position of the Circle Centre wrt agent\n",
    "        info_lidar: Has info if the agent is within or out of limits\n",
    "    '''\n",
    "    x_indices = np.where(lidar_values>0)[0]\n",
    "    if len(x_indices)>3:\n",
    "        raise\n",
    "        # We assumed that the lidar generates only 3 non-zero values, get_x_vals() uses this assumption\n",
    "\n",
    "    if len(x_indices)>0:\n",
    "        y_vals = lidar_values[x_indices]\n",
    "        x_vals = get_x_vals(x_indices,lidar_resolution)\n",
    "\n",
    "        a,b,c = np.polyfit(x_vals,y_vals,2)\n",
    "        theta_lidar = (-b/(2*a)) # Its in degrees\n",
    "        lidar_max = c - b**2/(4*a)\n",
    "        r = (1-lidar_max)*max_lidar_distance\n",
    "\n",
    "        theta_lidar = scale_angle(theta_lidar)+180/lidar_resolution\n",
    "        info_lidar = {'x_vals':x_vals,'y_vals':y_vals,'within_limits':True}\n",
    "\n",
    "    else:\n",
    "        theta_lidar = -1\n",
    "        r = max_lidar_distance\n",
    "        info_lidar = {'x_vals':-1,'y_vals':-1,'within_limits':False}\n",
    "\n",
    "    theta_lidar_rad = theta_lidar*np.pi/180\n",
    "    return theta_lidar_rad,r,info_lidar\n",
    "\n",
    "def get_coords(obs,max_lidar_distance,lidar_resolution):\n",
    "    '''Processes the 16-dim observation vector and return (x,y,theta) coordinates of the agent\n",
    "    by using the magnetometer for orientation and lidar for distance and angle from the origin\n",
    "    '''\n",
    "    \n",
    "    # Processing the State\n",
    "    lidar_values = obs[-16:]\n",
    "    mag0 = obs[9]\n",
    "    mag1 = obs[10]\n",
    "    theta_local = np.arctan2(mag0,mag1)\n",
    "    # theta_lidar = (np.argmax(lidar_values)+1)*2*np.pi/lidar_resolution\n",
    "    theta_lidar,r,info_lidar = get_lidar_r_theta(lidar_values,lidar_resolution,max_lidar_distance)\n",
    "    \n",
    "    if info_lidar['within_limits']==False:\n",
    "        # print(\"Agent OUTSIDE LIMITS\")\n",
    "        return -1,-1,-1,info_lidar\n",
    "\n",
    "    r = (1-max(lidar_values))*max_lidar_distance\n",
    "\n",
    "    theta_bot = theta_lidar+theta_local+np.pi\n",
    "    theta_bot_scaled = scale_angle(theta_bot*180/np.pi)*np.pi/180 #This is to bring it back to [0,2pi]\n",
    "\n",
    "    x,y = r*np.cos(theta_bot_scaled),r*np.sin(theta_bot_scaled)\n",
    "    # print(\"X Vals: {} | Y vals : {} | Theta Lidar: {:.2f} | Theta Local: {:.2f} | theta_bot_scaled :{:.2f} \".format(np.round(info_lidar['x_vals'],2),np.round(info_lidar['y_vals'],2),theta_lidar*180/np.pi,theta_local*180/np.pi,theta_bot_scaled*180/np.pi))\n",
    "\n",
    "    return x,y,theta_local,info_lidar\n",
    "\n",
    "def plot_trajectories(plot_trajectories_df,truly_safe_bounds,marginally_safe_bounds = [-1.1,1.1],ylim = 3,linewidth=0.1):\n",
    "    episode_arr = plot_trajectories_df.Episode.unique()\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    for episode in episode_arr:\n",
    "        trajectory_df = plot_trajectories_df[plot_trajectories_df.Episode==episode]\n",
    "        ax.plot(plot_trajectories_df.x,plot_trajectories_df.y,color='b',linewidth=linewidth)\n",
    "\n",
    "\n",
    "    # Plotting Truly Safe Set, Marginally Safe Set and Maximum Rewarding Circle\n",
    "    ax.plot([truly_safe_bounds[0],truly_safe_bounds[0]],[-ylim,ylim],color='g',linewidth=3)\n",
    "    ax.plot([truly_safe_bounds[1],truly_safe_bounds[1]],[-ylim,ylim],color='g',linewidth=3)\n",
    "    ax.plot([marginally_safe_bounds[0],marginally_safe_bounds[0]],[-ylim,ylim],color='r',linewidth=3)\n",
    "    ax.plot([marginally_safe_bounds[1],marginally_safe_bounds[1]],[-ylim,ylim],color='r',linewidth=3)\n",
    "    ax.add_patch(Circle((0, 0), 1.5, fill=False))\n",
    "\n",
    "    ax.set_xlim([-2,2])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "def print_logging_dict(logging_dict):\n",
    "\n",
    "    for key in logging_dict:\n",
    "        print(\"{}:{:.2f}\".format(key,logging_dict[key]),end=' | ')\n",
    "\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training PPO using Stable Baselines Environment\n",
    "Proximal Policy Algorithm (PPO) from the Stable Baselines Environment was used to train the agent to assess how a pure RL algorithm would perform. The agent in this case is indifferent to the safety contraints. The implementation using Stable Baselines is fairly straightforward.\n",
    "\n",
    "Make sure to use the \"make_gymnasium_environment\" method to create the gym environment compatible with the stable baselines model. The usual safety_gymnasium environments created using \"safety_gymnasium.make\" has a slightly different .step() function. In the former, <em>cost</em> is returned inside <em>info</em> while the <em>cost</em> is returned as a seperate variable in the latter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canvas(lidar_values,image_size=(256,256,3)):\n",
    "    roi_space = 1\n",
    "    x_pixel_range = image_size[0]*roi_space\n",
    "    y_pixel_range = image_size[1]*roi_space\n",
    "    max_lidar = 6\n",
    "    radius = int(0.01*x_pixel_range)\n",
    "    thickness = int(0.01*x_pixel_range)\n",
    "    color = (255,0,0)\n",
    "\n",
    "    canvas = np.ones(image_size,dtype=np.uint8)*255\n",
    "    angles = np.linspace(0,2*np.pi,len(lidar_values))\n",
    "\n",
    "    lidar_pixel_points = []\n",
    "    for angle,lidar_value in zip(angles,lidar_values):\n",
    "\n",
    "        # if lidar_value==0:\n",
    "        #     lidar_value = max_lidar\n",
    "\n",
    "        x_euclidean = lidar_value*np.cos(angle)\n",
    "        y_euclidean = lidar_value*np.sin(angle)\n",
    "\n",
    "        x_pixel = int(x_pixel_range/2*x_euclidean/max_lidar+x_pixel_range/2)\n",
    "        y_pixel = int(y_pixel_range/2 - y_pixel_range/2*y_euclidean/max_lidar)\n",
    "\n",
    "        # print(x_euclidean,y_euclidean,x_pixel,y_pixel)\n",
    "\n",
    "        cv2.circle(canvas,(x_pixel,y_pixel),radius,color,thickness)\n",
    "        lidar_pixel_points.append([x_pixel,y_pixel])\n",
    "\n",
    "    pts = np.array(lidar_pixel_points).astype(np.int32).reshape((-1, 1, 2))\n",
    "    cv2.polylines(canvas,[pts],isClosed=True,color=(0,255,0),thickness=thickness)\n",
    "\n",
    "    return canvas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 100000,\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "}\n",
    "run = wandb.init(\n",
    "    project=\"sb3\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "\n",
    "# def make_env(env):\n",
    "#     env = gym.make(config[\"env_name\"])\n",
    "#     env = Monitor(env)  # record stats such as returns\n",
    "#     return env\n",
    "\n",
    "\n",
    "# env = DummyVecEnv([make_env])\n",
    "# env = VecVideoRecorder(\n",
    "#     env,\n",
    "#     f\"videos/{run.id}\",\n",
    "#     record_video_trigger=lambda x: x % 2000 == 0,\n",
    "#     video_length=200,\n",
    "# )\n",
    "\n",
    "agent_type = 'Point'\n",
    "env_code = 1\n",
    "env_name = 'Safety{}Circle{}Gymnasium-v0'.format(agent_type,env_code)\n",
    "env = make_gymnasium_environment(env_name,render_mode=None)\n",
    "\n",
    "model = PPO(config[\"policy_type\"], env, verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"models/{run.id}\",\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "run.finish()\n",
    "\n",
    "# model_save_path = 'Saved_Models/ppo_200k'\n",
    "# model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'Saved_Models/ppo_200k'\n",
    "model_loaded = PPO.load(model_save_path)\n",
    "\n",
    "obs,_ = env.reset()\n",
    "while True:\n",
    "    # act = env.action_space.sample()\n",
    "    action, _state = model_loaded.predict(obs, deterministic=True)      \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    lidar_values = 1- obs[-16:]\n",
    "    if (lidar_values>1).sum()>0:\n",
    "        break\n",
    "    canvas = get_canvas(lidar_values)\n",
    "    env.render()\n",
    "    cv2.imshow(\"lidar\",canvas)\n",
    "    # cv2.imshow(\"frame\",frame)\n",
    "\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"### Episode Terminated ###\")\n",
    "        observation, info = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert Data Collection and Behaviour Cloning\n",
    "\n",
    "This section was implemented to try the approach where expert human trajectories are recorded and safe behaviour is learnt from these trajectories. Behaviour Cloning is implemented. However, this approach suffered from compounding error and a heuristic based CSP is implemented later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_data_collection(env,trajectories_df, wandb_log=False,num_episodes = 10):\n",
    "    episode = 0\n",
    "\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    dummy_action = env.action_space.sample()\n",
    "    terminated, truncated = False, False\n",
    "    ep_ret, ep_cost = 0, 0\n",
    "    obs_len = env.observation_space.shape[0]\n",
    "    act_len = env.action_space.shape[0]\n",
    "    max_lidar_distance = 6\n",
    "    lidar_resolution = 16\n",
    "    df_row = np.zeros(len(trajectories_df.columns))\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(project='safety_gymnasium')\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode += 1\n",
    "        ep_ret, ep_cost = 0, 0\n",
    "        time_step = 0\n",
    "\n",
    "        while True:\n",
    "            time_step +=1\n",
    "            assert env.observation_space.contains(obs)\n",
    "            obs, reward, cost, terminated, truncated, info = env.step(dummy_action)\n",
    "\n",
    "            x,y,theta_local,info_lidar = get_coords(obs,max_lidar_distance,lidar_resolution)\n",
    "\n",
    "            if info_lidar['within_limits']==False:\n",
    "                observation, info = env.reset()\n",
    "                break\n",
    "            \n",
    "            # print(\"x,y,theta: ({:.2f},{:.2f},{:.2f})\".format(x,y,theta_local*180/np.pi))\n",
    "            df_row[0] = env_code\n",
    "            df_row[1] = episode\n",
    "            df_row[2] = time_step\n",
    "            df_row[3:3+obs_len] = obs\n",
    "            df_row[3+obs_len:3+obs_len+3] = np.array([x,y,theta_local*180/np.pi])\n",
    "            df_row[-2-act_len:-2] = info['action_keyboard']\n",
    "            df_row[-2] = reward\n",
    "            df_row[-1] = cost\n",
    "\n",
    "            # print(df_row)\n",
    "            trajectories_df.loc[len(trajectories_df.index)] = df_row\n",
    "\n",
    "            ep_ret += reward\n",
    "            ep_cost += cost\n",
    "\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(\"Episode: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,ep_ret,ep_cost))\n",
    "                observation, info = env.reset()\n",
    "\n",
    "                logging_dict = {'Episode':episode,\n",
    "                                'TimeStep':time_step,\n",
    "                                'Reward': ep_ret,\n",
    "                                'Cost':ep_cost\n",
    "                                }\n",
    "                \n",
    "                if wandb_log:\n",
    "                    wandb.log(logging_dict)\n",
    "                print_logging_dict(logging_dict)\n",
    "                break\n",
    "            \n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return trajectories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_type = 'Point'\n",
    "env_code = 1\n",
    "env_name = 'Safety{}Circle{}Debug-v0'.format(agent_type,env_code)\n",
    "env = safety_gymnasium.make(env_name, render_mode='human')\n",
    "trajectories_df = get_trajectories_df() # Creates an empty dataframe to record the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Reward: 41.88 | Cost: 118.00\n",
      "Episode:1.00 | TimeStep:500.00 | Reward:41.88 | Cost:118.00 | \n",
      "Episode: 2 | Reward: 50.81 | Cost: 195.00\n",
      "Episode:2.00 | TimeStep:500.00 | Reward:50.81 | Cost:195.00 | \n",
      "Episode: 3 | Reward: 53.63 | Cost: 208.00\n",
      "Episode:3.00 | TimeStep:500.00 | Reward:53.63 | Cost:208.00 | \n",
      "Episode: 4 | Reward: 52.14 | Cost: 175.00\n",
      "Episode:4.00 | TimeStep:500.00 | Reward:52.14 | Cost:175.00 | \n",
      "Episode: 5 | Reward: 51.56 | Cost: 218.00\n",
      "Episode:5.00 | TimeStep:500.00 | Reward:51.56 | Cost:218.00 | \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>env_code</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Time_Step</th>\n",
       "      <th>obs_accelerometer_0</th>\n",
       "      <th>obs_accelerometer_1</th>\n",
       "      <th>obs_accelerometer_2</th>\n",
       "      <th>obs_velocimeter_0</th>\n",
       "      <th>obs_velocimeter_1</th>\n",
       "      <th>obs_velocimeter_2</th>\n",
       "      <th>obs_gyro_0</th>\n",
       "      <th>...</th>\n",
       "      <th>obs_circle_lidar_13</th>\n",
       "      <th>obs_circle_lidar_14</th>\n",
       "      <th>obs_circle_lidar_15</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>act_force_linear</th>\n",
       "      <th>act_angular_velocity</th>\n",
       "      <th>reward</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>-0.526693</td>\n",
       "      <td>34.740927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>-0.526693</td>\n",
       "      <td>34.740927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>-0.526693</td>\n",
       "      <td>34.740927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>-0.526693</td>\n",
       "      <td>34.740927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>-0.526693</td>\n",
       "      <td>34.740927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>1.897972</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.934861</td>\n",
       "      <td>-0.984052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.246304</td>\n",
       "      <td>-1.254880</td>\n",
       "      <td>-12.921978</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>1.321897</td>\n",
       "      <td>1.929434</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.901818</td>\n",
       "      <td>-1.000443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.232122</td>\n",
       "      <td>-1.277892</td>\n",
       "      <td>-9.527660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102692</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>1.384780</td>\n",
       "      <td>1.956012</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.869156</td>\n",
       "      <td>-1.014296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.217293</td>\n",
       "      <td>-1.300234</td>\n",
       "      <td>-6.134043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101821</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>1.446695</td>\n",
       "      <td>1.977944</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.837002</td>\n",
       "      <td>-1.025736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.201832</td>\n",
       "      <td>-1.321860</td>\n",
       "      <td>-2.741016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100927</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.507418</td>\n",
       "      <td>1.995472</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.805471</td>\n",
       "      <td>-1.034888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.185758</td>\n",
       "      <td>-1.342728</td>\n",
       "      <td>0.651526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      env_code  Episode  Time_Step  obs_accelerometer_0  obs_accelerometer_1  \\\n",
       "0          1.0      1.0        2.0             0.000000             0.000000   \n",
       "1          1.0      1.0        2.0             0.000000             0.000000   \n",
       "2          1.0      1.0        3.0             0.000000             0.000000   \n",
       "3          1.0      1.0        4.0             0.000000             0.000000   \n",
       "4          1.0      1.0        5.0             0.000000             0.000000   \n",
       "...        ...      ...        ...                  ...                  ...   \n",
       "2495       1.0      5.0      496.0             1.258293             1.897972   \n",
       "2496       1.0      5.0      497.0             1.321897             1.929434   \n",
       "2497       1.0      5.0      498.0             1.384780             1.956012   \n",
       "2498       1.0      5.0      499.0             1.446695             1.977944   \n",
       "2499       1.0      5.0      500.0             1.507418             1.995472   \n",
       "\n",
       "      obs_accelerometer_2  obs_velocimeter_0  obs_velocimeter_1  \\\n",
       "0                    9.81           0.000000           0.000000   \n",
       "1                    9.81           0.000000           0.000000   \n",
       "2                    9.81           0.000000           0.000000   \n",
       "3                    9.81           0.000000           0.000000   \n",
       "4                    9.81           0.000000           0.000000   \n",
       "...                   ...                ...                ...   \n",
       "2495                 9.81           0.934861          -0.984052   \n",
       "2496                 9.81           0.901818          -1.000443   \n",
       "2497                 9.81           0.869156          -1.014296   \n",
       "2498                 9.81           0.837002          -1.025736   \n",
       "2499                 9.81           0.805471          -1.034888   \n",
       "\n",
       "      obs_velocimeter_2  obs_gyro_0  ...  obs_circle_lidar_13  \\\n",
       "0                   0.0         0.0  ...                  0.0   \n",
       "1                   0.0         0.0  ...                  0.0   \n",
       "2                   0.0         0.0  ...                  0.0   \n",
       "3                   0.0         0.0  ...                  0.0   \n",
       "4                   0.0         0.0  ...                  0.0   \n",
       "...                 ...         ...  ...                  ...   \n",
       "2495                0.0         0.0  ...                  0.0   \n",
       "2496                0.0         0.0  ...                  0.0   \n",
       "2497                0.0         0.0  ...                  0.0   \n",
       "2498                0.0         0.0  ...                  0.0   \n",
       "2499                0.0         0.0  ...                  0.0   \n",
       "\n",
       "      obs_circle_lidar_14  obs_circle_lidar_15         x         y      theta  \\\n",
       "0                     0.0                  0.0  0.037692 -0.526693  34.740927   \n",
       "1                     0.0                  0.0  0.037692 -0.526693  34.740927   \n",
       "2                     0.0                  0.0  0.037692 -0.526693  34.740927   \n",
       "3                     0.0                  0.0  0.037692 -0.526693  34.740927   \n",
       "4                     0.0                  0.0  0.037692 -0.526693  34.740927   \n",
       "...                   ...                  ...       ...       ...        ...   \n",
       "2495                  0.0                  0.0 -1.246304 -1.254880 -12.921978   \n",
       "2496                  0.0                  0.0 -1.232122 -1.277892  -9.527660   \n",
       "2497                  0.0                  0.0 -1.217293 -1.300234  -6.134043   \n",
       "2498                  0.0                  0.0 -1.201832 -1.321860  -2.741016   \n",
       "2499                  0.0                  0.0 -1.185758 -1.342728   0.651526   \n",
       "\n",
       "      act_force_linear  act_angular_velocity    reward  cost  \n",
       "0                  0.0                   0.0  0.000000   0.0  \n",
       "1                  0.0                   0.0  0.000000   0.0  \n",
       "2                  0.0                   0.0  0.000000   0.0  \n",
       "3                  0.0                   0.0  0.000000   0.0  \n",
       "4                  0.0                   0.0  0.000000   0.0  \n",
       "...                ...                   ...       ...   ...  \n",
       "2495               1.0                   1.0  0.103544   1.0  \n",
       "2496               1.0                   1.0  0.102692   1.0  \n",
       "2497               1.0                   1.0  0.101821   1.0  \n",
       "2498               1.0                   1.0  0.100927   1.0  \n",
       "2499               1.0                   1.0  0.100008   1.0  \n",
       "\n",
       "[2500 rows x 38 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_data_collection(env,trajectories_df,wandb_log=False,num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>env_code</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Time_Step</th>\n",
       "      <th>obs_accelerometer_0</th>\n",
       "      <th>obs_accelerometer_1</th>\n",
       "      <th>obs_accelerometer_2</th>\n",
       "      <th>obs_velocimeter_0</th>\n",
       "      <th>obs_velocimeter_1</th>\n",
       "      <th>obs_velocimeter_2</th>\n",
       "      <th>obs_gyro_0</th>\n",
       "      <th>...</th>\n",
       "      <th>obs_circle_lidar_13</th>\n",
       "      <th>obs_circle_lidar_14</th>\n",
       "      <th>obs_circle_lidar_15</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>theta</th>\n",
       "      <th>act_force_linear</th>\n",
       "      <th>act_angular_velocity</th>\n",
       "      <th>reward</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.329798</td>\n",
       "      <td>81.804773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.329798</td>\n",
       "      <td>81.804773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.329798</td>\n",
       "      <td>81.804773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.329798</td>\n",
       "      <td>81.804773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612942</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.329798</td>\n",
       "      <td>81.804773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>0.572323</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.398974</td>\n",
       "      <td>-0.302811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735353</td>\n",
       "      <td>0.168280</td>\n",
       "      <td>103.873891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>0.390991</td>\n",
       "      <td>0.722405</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.386313</td>\n",
       "      <td>-0.373213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734371</td>\n",
       "      <td>0.196939</td>\n",
       "      <td>107.296529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080480</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>0.421911</td>\n",
       "      <td>0.851554</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.370123</td>\n",
       "      <td>-0.439995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733069</td>\n",
       "      <td>0.225662</td>\n",
       "      <td>110.717264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>0.458951</td>\n",
       "      <td>0.973133</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.350757</td>\n",
       "      <td>-0.503146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731390</td>\n",
       "      <td>0.254421</td>\n",
       "      <td>114.135137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.501474</td>\n",
       "      <td>1.087628</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.328553</td>\n",
       "      <td>-0.562635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729283</td>\n",
       "      <td>0.283189</td>\n",
       "      <td>117.550264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080486</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4500 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      env_code  Episode  Time_Step  obs_accelerometer_0  obs_accelerometer_1  \\\n",
       "0          1.0      1.0        2.0             0.000000             0.000000   \n",
       "1          1.0      1.0        2.0             0.000000             0.000000   \n",
       "2          1.0      1.0        3.0             0.000000             0.000000   \n",
       "3          1.0      1.0        4.0             0.000000             0.000000   \n",
       "4          1.0      1.0        5.0             0.000000             0.000000   \n",
       "...        ...      ...        ...                  ...                  ...   \n",
       "5495       1.0     10.0      496.0             0.366448             0.572323   \n",
       "5496       1.0     10.0      497.0             0.390991             0.722405   \n",
       "5497       1.0     10.0      498.0             0.421911             0.851554   \n",
       "5498       1.0     10.0      499.0             0.458951             0.973133   \n",
       "5499       1.0     10.0      500.0             0.501474             1.087628   \n",
       "\n",
       "      obs_accelerometer_2  obs_velocimeter_0  obs_velocimeter_1  \\\n",
       "0                    9.81           0.000000           0.000000   \n",
       "1                    9.81           0.000000           0.000000   \n",
       "2                    9.81           0.000000           0.000000   \n",
       "3                    9.81           0.000000           0.000000   \n",
       "4                    9.81           0.000000           0.000000   \n",
       "...                   ...                ...                ...   \n",
       "5495                 9.81           1.398974          -0.302811   \n",
       "5496                 9.81           1.386313          -0.373213   \n",
       "5497                 9.81           1.370123          -0.439995   \n",
       "5498                 9.81           1.350757          -0.503146   \n",
       "5499                 9.81           1.328553          -0.562635   \n",
       "\n",
       "      obs_velocimeter_2  obs_gyro_0  ...  obs_circle_lidar_13  \\\n",
       "0                   0.0         0.0  ...                  0.0   \n",
       "1                   0.0         0.0  ...                  0.0   \n",
       "2                   0.0         0.0  ...                  0.0   \n",
       "3                   0.0         0.0  ...                  0.0   \n",
       "4                   0.0         0.0  ...                  0.0   \n",
       "...                 ...         ...  ...                  ...   \n",
       "5495                0.0         0.0  ...                  0.0   \n",
       "5496                0.0         0.0  ...                  0.0   \n",
       "5497                0.0         0.0  ...                  0.0   \n",
       "5498                0.0         0.0  ...                  0.0   \n",
       "5499                0.0         0.0  ...                  0.0   \n",
       "\n",
       "      obs_circle_lidar_14  obs_circle_lidar_15         x         y  \\\n",
       "0                     0.0             0.612942 -0.001661 -0.329798   \n",
       "1                     0.0             0.612942 -0.001661 -0.329798   \n",
       "2                     0.0             0.612942 -0.001661 -0.329798   \n",
       "3                     0.0             0.612942 -0.001661 -0.329798   \n",
       "4                     0.0             0.612942 -0.001661 -0.329798   \n",
       "...                   ...                  ...       ...       ...   \n",
       "5495                  0.0             0.000000  0.735353  0.168280   \n",
       "5496                  0.0             0.000000  0.734371  0.196939   \n",
       "5497                  0.0             0.000000  0.733069  0.225662   \n",
       "5498                  0.0             0.000000  0.731390  0.254421   \n",
       "5499                  0.0             0.000000  0.729283  0.283189   \n",
       "\n",
       "           theta  act_force_linear  act_angular_velocity    reward  cost  \n",
       "0      81.804773               0.0                   0.0  0.000000   0.0  \n",
       "1      81.804773               0.0                   0.0  0.000000   0.0  \n",
       "2      81.804773               0.0                   0.0  0.000000   0.0  \n",
       "3      81.804773               0.0                   0.0  0.000000   0.0  \n",
       "4      81.804773               0.0                   0.0  0.000000   0.0  \n",
       "...          ...               ...                   ...       ...   ...  \n",
       "5495  103.873891               1.0                   1.0  0.080427   0.0  \n",
       "5496  107.296529               1.0                   1.0  0.080480   0.0  \n",
       "5497  110.717264               1.0                   1.0  0.080502   0.0  \n",
       "5498  114.135137               1.0                   1.0  0.080502   0.0  \n",
       "5499  117.550264               1.0                   1.0  0.080486   0.0  \n",
       "\n",
       "[4500 rows x 38 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsafe_episodes = trajectories_df[trajectories_df.cost==1].Episode.unique()\n",
    "safe_trajectories_df = trajectories_df[~trajectories_df.Episode.isin(unsafe_episodes)]\n",
    "safe_trajectories_df = safe_trajectories_df[abs(safe_trajectories_df.y)<3]\n",
    "\n",
    "safe_trajectories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "truly_safe_bounds = [-0.8,0.8]\n",
    "marginally_safe_bounds = [-1.1,1.1]\n",
    "\n",
    "\n",
    "plot_trajectories_df = trajectories_df\n",
    "\n",
    "plot_trajectories(plot_trajectories_df,truly_safe_bounds,linewidth=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Behavior Cloning to Learn Safe Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = safe_trajectories_df[abs(safe_trajectories_df.x)>0.7]\n",
    "expert_observations = df[[col for col in df.columns if 'obs_' in col]].values\n",
    "expert_actions = df[[col for col in df.columns if 'act_' in col]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = len(safe_trajectories_df)\n",
    "# expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "# expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "np.savez_compressed(\n",
    "   \"test_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataSet(Dataset):\n",
    "   def __init__(self, expert_observations, expert_actions):\n",
    "      self.observations = expert_observations.astype(np.float32)\n",
    "      self.actions = expert_actions.astype(np.float32)\n",
    "   def __getitem__(self, index):\n",
    "      return (self.observations[index], self.actions[index])\n",
    "   def __len__(self):\n",
    "      return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "test_size = len(expert_dataset) - train_size\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "     expert_dataset, [train_size, test_size]\n",
    "     )\n",
    "\n",
    "train_loader = DataLoader(train_expert_dataset,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_expert_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define your model\n",
    "class CSP_NN(nn.Module):\n",
    "    def __init__(self,num_input,num_output):\n",
    "        super(CSP_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_input, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "\n",
    "        self.fc4 = nn.Linear(64, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = F.tanh(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "num_input = 28\n",
    "num_output = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.12609233463352376 | Test Loss: 0.04494369216263294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.0004140376668973741 | Test Loss: 0.0003655812771674911\n",
      "Epoch: 20 | Train Loss: 0.00015858549476516519 | Test Loss: 0.00013439297513936514\n",
      "Epoch: 30 | Train Loss: 7.040246266776442e-05 | Test Loss: 0.00015120511410974098\n",
      "Epoch: 40 | Train Loss: 0.0001916980532273142 | Test Loss: 0.00012825910796111981\n",
      "Epoch: 50 | Train Loss: 0.0006444972038893452 | Test Loss: 0.00028485663018987646\n",
      "Epoch: 60 | Train Loss: 6.867222929775546e-05 | Test Loss: 0.0001325254644533353\n",
      "Epoch: 70 | Train Loss: 0.00027288921191939155 | Test Loss: 0.00048475529599402634\n",
      "Epoch: 80 | Train Loss: 0.0010714801390845836 | Test Loss: 0.0011556241744463997\n",
      "Epoch: 90 | Train Loss: 2.9517745910693934e-05 | Test Loss: 3.247765783141533e-05\n"
     ]
    }
   ],
   "source": [
    "def train(model,train_loader,test_loader,num_epochs=100,lr = 0.001,print_every=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        if epoch%print_every==0:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                epoch_loss_test = 0\n",
    "                for x,y in test_loader:\n",
    "                    # optimizer.zero_grad()\n",
    "                    y_hat = model(x)\n",
    "                    loss = criterion(y_hat,y)\n",
    "                    # loss.backward()\n",
    "                    # optimizer.step()\n",
    "\n",
    "                    epoch_loss_test += loss.item()\n",
    "\n",
    "            print(\"Epoch: {} | Train Loss: {} | Test Loss: {}\".format(epoch,epoch_loss/len(train_loader),epoch_loss_test/len(test_loader)))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = CSP_NN(num_input,num_output)\n",
    "model = train(model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom CSP Definition (Heuristic Based)\n",
    "\n",
    "This implementation is for the SafetyCircle Environment only. However, similar heuristic based CSPs can be developed for other safety_gymnasium environments like SafeNavigation where obstacles with different costs need to be avoided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_type = 'Point'\n",
    "env_code = 1\n",
    "env_name = 'Safety{}Circle{}Gymnasium-v0'.format(agent_type,env_code)\n",
    "env = make_gymnasium_environment(env_name,render_mode=None)\n",
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csp_action(x,y,theta_local_rad,csp_config):\n",
    "\n",
    "    truly_safe_set_bound = csp_config['truly_safe_set_bound']\n",
    "    y_bound = csp_config['y_bound']\n",
    "    linear_vel_angle = csp_config['linear_vel_angle']\n",
    "    no_turn_angle = csp_config['no_turn_angle']\n",
    "    marginally_safe_set_bound = csp_config['marginally_safe_set_bound']\n",
    "\n",
    "\n",
    "    theta_local = theta_local_rad*180/np.pi\n",
    "    if abs(x)<=truly_safe_set_bound:\n",
    "        act = np.array([0.1,np.random.uniform(-1,1)]) \n",
    "        # Inside the Truly Safe State Space, the agent is just made to go forward , hence the 0.1 (it can be any other action) \n",
    "        # we need our csp to act primarily in the marginally safe set, we do not care about its actions inside the truly safe set\n",
    "\n",
    "    else:\n",
    "        if x>truly_safe_set_bound and ((0<theta_local<180-no_turn_angle) or (180+no_turn_angle<theta_local<360)):\n",
    "            if 0<theta_local<linear_vel_angle or (360-linear_vel_angle<theta_local<360):\n",
    "                act_linear = -0.1\n",
    "            else:\n",
    "                act_linear = 0\n",
    "\n",
    "            if (0<theta_local<160):\n",
    "                act_angular = np.random.uniform(0.8,1)\n",
    "            elif 200<theta_local<360:\n",
    "                act_angular = -np.random.uniform(0.8,1)\n",
    "            \n",
    "            act = np.array([act_linear,act_angular])\n",
    "\n",
    "            \n",
    "\n",
    "        elif x<-truly_safe_set_bound and no_turn_angle<theta_local<360-no_turn_angle:\n",
    "            if 180-linear_vel_angle<theta_local<180+linear_vel_angle:\n",
    "                act_linear=-0.1\n",
    "            else:\n",
    "                act_linear = 0\n",
    "\n",
    "            if 10<theta_local<180:\n",
    "                act_angular = -np.random.uniform(0.8,1)\n",
    "            else:\n",
    "                act_angular = np.random.uniform(0.8,1)\n",
    "            \n",
    "            act = np.array([act_linear,act_angular])\n",
    "      \n",
    "        else:\n",
    "            act = np.array([0.8,0])\n",
    "\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 201 | Time Step: 500 | Reward: 3.35 | Cost: 0.00\n",
      "Episode: 202 | Time Step: 500 | Reward: -1.20 | Cost: 0.00\n",
      "Episode: 203 | Time Step: 500 | Reward: 1.40 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 204 | Time Step: 466 | Reward: -0.59 | Cost: 3.00\n",
      "Episode: 205 | Time Step: 500 | Reward: -6.65 | Cost: 0.00\n",
      "Episode: 206 | Time Step: 500 | Reward: -3.48 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 207 | Time Step: 256 | Reward: 1.38 | Cost: 0.00\n",
      "Episode: 208 | Time Step: 500 | Reward: 4.50 | Cost: 0.00\n",
      "Episode: 209 | Time Step: 500 | Reward: 6.57 | Cost: 0.00\n",
      "Episode: 210 | Time Step: 500 | Reward: 4.23 | Cost: 0.00\n",
      "Episode: 211 | Time Step: 500 | Reward: 5.36 | Cost: 0.00\n",
      "Episode: 212 | Time Step: 500 | Reward: 0.70 | Cost: 0.00\n",
      "Episode: 213 | Time Step: 500 | Reward: 3.59 | Cost: 0.00\n",
      "Episode: 214 | Time Step: 500 | Reward: -2.41 | Cost: 0.00\n",
      "Episode: 215 | Time Step: 500 | Reward: 4.65 | Cost: 0.00\n",
      "Episode: 216 | Time Step: 500 | Reward: 5.64 | Cost: 0.00\n",
      "Episode: 217 | Time Step: 500 | Reward: 3.30 | Cost: 0.00\n",
      "Episode: 218 | Time Step: 500 | Reward: -2.79 | Cost: 0.00\n",
      "Episode: 219 | Time Step: 500 | Reward: 5.81 | Cost: 0.00\n",
      "Episode: 220 | Time Step: 500 | Reward: 3.19 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 221 | Time Step: 243 | Reward: 2.06 | Cost: 0.00\n",
      "Episode: 222 | Time Step: 500 | Reward: -1.89 | Cost: 0.00\n",
      "Episode: 223 | Time Step: 500 | Reward: 1.67 | Cost: 11.00\n",
      "Episode: 224 | Time Step: 500 | Reward: 3.38 | Cost: 0.00\n",
      "Episode: 225 | Time Step: 500 | Reward: 2.87 | Cost: 11.00\n",
      "Episode: 226 | Time Step: 500 | Reward: 2.02 | Cost: 0.00\n",
      "Episode: 227 | Time Step: 500 | Reward: 2.31 | Cost: 8.00\n",
      "Episode: 228 | Time Step: 500 | Reward: -1.79 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 229 | Time Step: 253 | Reward: 5.69 | Cost: 0.00\n",
      "Episode: 230 | Time Step: 500 | Reward: -6.61 | Cost: 0.00\n",
      "Episode: 231 | Time Step: 500 | Reward: -1.31 | Cost: 0.00\n",
      "Episode: 232 | Time Step: 500 | Reward: 1.32 | Cost: 0.00\n",
      "Episode: 233 | Time Step: 500 | Reward: -5.98 | Cost: 0.00\n",
      "Episode: 234 | Time Step: 500 | Reward: 2.98 | Cost: 0.00\n",
      "Episode: 235 | Time Step: 500 | Reward: 4.49 | Cost: 0.00\n",
      "Episode: 236 | Time Step: 500 | Reward: 2.08 | Cost: 0.00\n",
      "Episode: 237 | Time Step: 500 | Reward: -1.33 | Cost: 0.00\n",
      "Episode: 238 | Time Step: 500 | Reward: -7.75 | Cost: 0.00\n",
      "Episode: 239 | Time Step: 500 | Reward: 1.15 | Cost: 0.00\n",
      "Episode: 240 | Time Step: 500 | Reward: -4.03 | Cost: 0.00\n",
      "Episode: 241 | Time Step: 500 | Reward: -2.07 | Cost: 0.00\n",
      "Episode: 242 | Time Step: 500 | Reward: 0.32 | Cost: 0.00\n",
      "Episode: 243 | Time Step: 500 | Reward: 1.15 | Cost: 0.00\n",
      "Episode: 244 | Time Step: 500 | Reward: -2.81 | Cost: 0.00\n",
      "Episode: 245 | Time Step: 500 | Reward: 4.39 | Cost: 0.00\n",
      "Episode: 246 | Time Step: 500 | Reward: 4.35 | Cost: 0.00\n",
      "Episode: 247 | Time Step: 500 | Reward: 5.03 | Cost: 0.00\n",
      "Episode: 248 | Time Step: 500 | Reward: 4.56 | Cost: 0.00\n",
      "Episode: 249 | Time Step: 500 | Reward: -1.78 | Cost: 12.00\n",
      "Episode: 250 | Time Step: 500 | Reward: 0.94 | Cost: 0.00\n",
      "Episode: 251 | Time Step: 500 | Reward: 6.27 | Cost: 0.00\n",
      "Episode: 252 | Time Step: 500 | Reward: 4.22 | Cost: 0.00\n",
      "Episode: 253 | Time Step: 500 | Reward: -4.54 | Cost: 0.00\n",
      "Episode: 254 | Time Step: 500 | Reward: -3.47 | Cost: 0.00\n",
      "Episode: 255 | Time Step: 500 | Reward: -3.35 | Cost: 4.00\n",
      "Episode: 256 | Time Step: 500 | Reward: -1.54 | Cost: 0.00\n",
      "Episode: 257 | Time Step: 500 | Reward: -5.65 | Cost: 0.00\n",
      "Episode: 258 | Time Step: 500 | Reward: 0.35 | Cost: 0.00\n",
      "Episode: 259 | Time Step: 500 | Reward: 3.61 | Cost: 0.00\n",
      "Episode: 260 | Time Step: 500 | Reward: -3.07 | Cost: 0.00\n",
      "Episode: 261 | Time Step: 500 | Reward: 1.35 | Cost: 0.00\n",
      "Episode: 262 | Time Step: 500 | Reward: -6.52 | Cost: 9.00\n",
      "Episode: 263 | Time Step: 500 | Reward: -5.12 | Cost: 0.00\n",
      "Episode: 264 | Time Step: 500 | Reward: -2.36 | Cost: 0.00\n",
      "Episode: 265 | Time Step: 500 | Reward: -1.87 | Cost: 0.00\n",
      "Episode: 266 | Time Step: 500 | Reward: -1.44 | Cost: 0.00\n",
      "Episode: 267 | Time Step: 500 | Reward: -2.57 | Cost: 0.00\n",
      "Episode: 268 | Time Step: 500 | Reward: 6.31 | Cost: 0.00\n",
      "Episode: 269 | Time Step: 500 | Reward: -4.33 | Cost: 0.00\n",
      "Episode: 270 | Time Step: 500 | Reward: 2.68 | Cost: 0.00\n",
      "Episode: 271 | Time Step: 500 | Reward: 6.40 | Cost: 10.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 272 | Time Step: 367 | Reward: 0.73 | Cost: 0.00\n",
      "Episode: 273 | Time Step: 500 | Reward: 0.56 | Cost: 0.00\n",
      "Episode: 274 | Time Step: 500 | Reward: 0.52 | Cost: 0.00\n",
      "Episode: 275 | Time Step: 500 | Reward: 4.08 | Cost: 0.00\n",
      "Episode: 276 | Time Step: 500 | Reward: 6.45 | Cost: 0.00\n",
      "Episode: 277 | Time Step: 500 | Reward: -2.22 | Cost: 0.00\n",
      "Episode: 278 | Time Step: 500 | Reward: -0.53 | Cost: 0.00\n",
      "Episode: 279 | Time Step: 500 | Reward: 4.04 | Cost: 0.00\n",
      "Episode: 280 | Time Step: 500 | Reward: -6.28 | Cost: 0.00\n",
      "Episode: 281 | Time Step: 500 | Reward: 7.34 | Cost: 0.00\n",
      "Episode: 282 | Time Step: 500 | Reward: -0.00 | Cost: 0.00\n",
      "Episode: 283 | Time Step: 500 | Reward: -4.51 | Cost: 0.00\n",
      "Episode: 284 | Time Step: 500 | Reward: 7.54 | Cost: 0.00\n",
      "Episode: 285 | Time Step: 500 | Reward: 2.58 | Cost: 12.00\n",
      "Episode: 286 | Time Step: 500 | Reward: 3.76 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 287 | Time Step: 250 | Reward: 4.74 | Cost: 0.00\n",
      "Episode: 288 | Time Step: 500 | Reward: -4.40 | Cost: 0.00\n",
      "Episode: 289 | Time Step: 500 | Reward: 5.58 | Cost: 0.00\n",
      "Episode: 290 | Time Step: 500 | Reward: 4.44 | Cost: 0.00\n",
      "Episode: 291 | Time Step: 500 | Reward: 3.62 | Cost: 0.00\n",
      "Episode: 292 | Time Step: 500 | Reward: -5.21 | Cost: 0.00\n",
      "Episode: 293 | Time Step: 500 | Reward: 3.66 | Cost: 0.00\n",
      "Episode: 294 | Time Step: 500 | Reward: -2.57 | Cost: 0.00\n",
      "Episode: 295 | Time Step: 500 | Reward: 3.61 | Cost: 0.00\n",
      "AGENT OUTSIDE LIMITS | Episode: 296 | Time Step: 220 | Reward: 2.56 | Cost: 0.00\n",
      "Episode: 297 | Time Step: 500 | Reward: -5.57 | Cost: 0.00\n",
      "Episode: 298 | Time Step: 500 | Reward: 1.14 | Cost: 0.00\n",
      "Episode: 299 | Time Step: 500 | Reward: -0.98 | Cost: 0.00\n",
      "Episode: 300 | Time Step: 500 | Reward: -3.84 | Cost: 0.00\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "dummy_action = env.action_space.sample()\n",
    "terminated, truncated = False, False\n",
    "ep_ret, ep_cost = 0, 0\n",
    "obs_len = env.observation_space.shape[0]\n",
    "act_len = env.action_space.shape[0]\n",
    "max_lidar_distance = 6\n",
    "lidar_resolution = 16\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "csp_config = {'truly_safe_set_bound':0.6,\n",
    "              'marginally_safe_set_bound':1.1,\n",
    "            'y_bound':2,\n",
    "            'linear_vel_angle':40, # Within 40 degrees of pointing to the obstacle, there will be a negative linear force\n",
    "            'no_turn_angle':20, # Within 20 degrees away from the wall, there will be no turning\n",
    "            'parallel_angle':10} \n",
    "\n",
    "trajectories_df = get_trajectories_df()\n",
    "df_row = np.zeros(len(trajectories_df.columns))\n",
    "for _ in range(num_episodes):\n",
    "    episode += 1\n",
    "    ep_ret, ep_cost = 0, 0\n",
    "    time_step = 0\n",
    "\n",
    "    while True:\n",
    "        time_step +=1\n",
    "        assert env.observation_space.contains(obs)\n",
    "        # act = env.action_space.sample()\n",
    "        \n",
    "        x,y,theta_local,info_lidar = get_coords(obs,max_lidar_distance,lidar_resolution)\n",
    "        theta_local = scale_angle(theta_local*180/np.pi)*np.pi/180\n",
    "        if info_lidar['within_limits']==False:\n",
    "            print(\"AGENT OUTSIDE LIMITS | Episode: {} | Time Step: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,time_step,ep_ret,ep_cost))\n",
    "            obs, info = env.reset()\n",
    "            break\n",
    "\n",
    "        act = get_csp_action(x,y,theta_local,csp_config)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(act)\n",
    "        # print(\"Time Step: {} | a1: {:.2f} | a2: {:.2f} | a3: {:.2f} | v1: {:.2f} | v2: {:.2f} | v3: {:.2f} | \".format(time_step,obs[0],obs[1],obs[2],obs[3],obs[4],obs[5]))\n",
    "        cost = info['cost']\n",
    "\n",
    "        ep_ret += reward\n",
    "        ep_cost += cost\n",
    "\n",
    "        # Logging\n",
    "        df_row[0] = env_code\n",
    "        df_row[1] = episode\n",
    "        df_row[2] = time_step\n",
    "        df_row[3:3+obs_len] = obs\n",
    "        df_row[3+obs_len:3+obs_len+3] = np.array([x,y,theta_local*180/np.pi])\n",
    "        df_row[-2-act_len:-2] = act\n",
    "        df_row[-2] = reward\n",
    "        df_row[-1] = cost\n",
    "\n",
    "        # print(df_row)\n",
    "        trajectories_df.loc[len(trajectories_df.index)] = df_row\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode: {} | Time Step: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,time_step,ep_ret,ep_cost))\n",
    "            obs, info = env.reset()\n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Safe RL with SSS and CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent:\n",
    "    def __init__(self, env,ddpg_agent_config_dict):\n",
    "        \n",
    "        # Params\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.gamma = ddpg_agent_config_dict['gamma']\n",
    "        self.tau = ddpg_agent_config_dict['tau']\n",
    "        self.weight_decay_critic = ddpg_agent_config_dict['weight_decay_critic']\n",
    "        self.weight_decay_actor = ddpg_agent_config_dict['weight_decay_actor']\n",
    "        self.max_memory_size = ddpg_agent_config_dict['max_memory_size']\n",
    "        self.actor_learning_rate = ddpg_agent_config_dict['actor_learning_rate']\n",
    "        self.critic_learning_rate = ddpg_agent_config_dict['critic_learning_rate']\n",
    "        self.hidden_size = ddpg_agent_config_dict['hidden_size']\n",
    "\n",
    "\n",
    "        # Networks\n",
    "        self.actor = Actor(self.num_states, self.hidden_size, self.num_actions)\n",
    "        self.actor_target = Actor(self.num_states, self.hidden_size, self.num_actions)\n",
    "        self.critic = Critic(self.num_states + self.num_actions, self.hidden_size, self.num_actions)\n",
    "        self.critic_target = Critic(self.num_states + self.num_actions, self.hidden_size, self.num_actions)\n",
    "\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Training\n",
    "        self.memory = Memory(self.max_memory_size)        \n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=self.actor_learning_rate, weight_decay=self.weight_decay_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_learning_rate,weight_decay=self.weight_decay_critic)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action = self.actor.forward(state)\n",
    "        action = action.detach().numpy()\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample(batch_size) #states = List of np arrays of shape (self.num_states,)\n",
    "        # print(type(states))\n",
    "        states = torch.Tensor(np.vstack(states))\n",
    "        actions = torch.Tensor(np.vstack(actions))\n",
    "        rewards = torch.Tensor(np.vstack(rewards))\n",
    "        next_states = torch.Tensor(np.vstack(next_states))\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals = self.critic.forward(states, actions) # shape = (batch_size, num_actions)\n",
    "        next_actions = self.actor_target.forward(next_states) # shape = (batch_size, num_actions)\n",
    "        next_Q = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        Qprime = rewards + self.gamma * next_Q\n",
    "        critic_loss = self.critic_criterion(Qvals, Qprime)\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss_before_mean = -self.critic.forward(states, self.actor.forward(states))\n",
    "        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "        \n",
    "        # update networks\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "        # return 0,0\n",
    "        # print(\"Qvals.shape: {} | Qprime.shape: {} | policy_loss_before_mean.shape: {} | critic_loss.shape: {}\".format(Qvals.shape,Qprime.shape,policy_loss_before_mean.shape,critic_loss.shape))\n",
    "        info_update = {'Qvals':Qvals.detach().norm()/Qvals.detach().numel(),\n",
    "                       'Qprime':Qprime.detach().norm()/Qprime.detach().numel()}\n",
    "        # info_update = {}\n",
    "        return policy_loss.detach().item(),critic_loss.detach().item(),info_update\n",
    "        \n",
    "    \n",
    "def print_logging_dict(logging_dict):\n",
    "\n",
    "    for key in logging_dict:\n",
    "        print(\"{}:{:.2f}\".format(key,logging_dict[key]),end=' | ')\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_action(action,x,y,theta_local_rad,csp_config):\n",
    "\n",
    "    truly_safe_set_bound = csp_config['truly_safe_set_bound']\n",
    "    y_bound = csp_config['y_bound']\n",
    "    linear_vel_angle = csp_config['linear_vel_angle']\n",
    "    no_turn_angle = csp_config['no_turn_angle']\n",
    "    parallel_angle = csp_config['parallel_angle']\n",
    "    marginally_safe_set_bound = csp_config['marginally_safe_set_bound']\n",
    "\n",
    "\n",
    "    theta_local = theta_local_rad*180/np.pi\n",
    "    if abs(x)<=truly_safe_set_bound:\n",
    "        act = action\n",
    "        # act = np.array([0.1,0])\n",
    "\n",
    "    # elif x>truly_safe_set_bound and x<marginally_safe_set_bound and 90+parallel_angle<theta_local<270-parallel_angle:\n",
    "    #     act = action\n",
    "    #     # print('here2')\n",
    "    # elif x<=-truly_safe_set_bound and x>-marginally_safe_set_bound and (0<theta_local<90-parallel_angle or 170+parallel_angle<theta_local<360):\n",
    "    #     act = action\n",
    "        # print('here3')\n",
    "\n",
    "    else:\n",
    "        if x>truly_safe_set_bound and ((0<theta_local<180-no_turn_angle) or (180+no_turn_angle<theta_local<360)):\n",
    "            if 0<theta_local<linear_vel_angle or (360-linear_vel_angle<theta_local<360):\n",
    "                act_linear = -0.1\n",
    "            else:\n",
    "                act_linear = 0\n",
    "\n",
    "            if (0<theta_local<160):\n",
    "                act_angular = np.random.uniform(0.8,1)\n",
    "            elif 200<theta_local<360:\n",
    "                act_angular = -np.random.uniform(0.8,1)\n",
    "            \n",
    "            act = np.array([act_linear,act_angular])\n",
    "            # act2 = np.array([-0.1,np.random.uniform(-1,-0.9)])\n",
    "            # act_i = np.random.choice([0,1],p=[0.9,0.1])\n",
    "            # act = [act1,act2][act_i]\n",
    "\n",
    "            # act = -2*obs[:2]#np.array([0,0])\n",
    "        # else:\n",
    "        #     act = np.array([0.5,0])\n",
    "        elif x<-truly_safe_set_bound and no_turn_angle<theta_local<360-no_turn_angle:\n",
    "            if 180-linear_vel_angle<theta_local<180+linear_vel_angle:\n",
    "                act_linear=-0.1\n",
    "            else:\n",
    "                act_linear = 0\n",
    "\n",
    "            if 10<theta_local<180:\n",
    "                act_angular = -np.random.uniform(0.8,1)\n",
    "            else:\n",
    "                act_angular = np.random.uniform(0.8,1)\n",
    "            \n",
    "            act = np.array([act_linear,act_angular])\n",
    "            # act2 = np.array([-0.10,np.random.uniform(-1,-0.9)])\n",
    "            # act_i = np.random.choice([0,1],p=[0.2,0.8])\n",
    "            # act = [act1,act2][act_i]            \n",
    "        else:\n",
    "            act = np.array([0.8,0])\n",
    "            # act = -2*obs[0:2]#np.array([0,0])\n",
    "\n",
    "    return act\n",
    "\n",
    "def train(agent,train_config_dict,csp_config):\n",
    "\n",
    "    wandb_log = train_config_dict['wandb_log']\n",
    "    num_episodes = train_config_dict['num_episodes']\n",
    "    num_timesteps = train_config_dict['num_timesteps']\n",
    "    epsilon = train_config_dict['epsilon']\n",
    "    batch_size = train_config_dict['batch_size']\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(project='safety_gymnasium',config=train_config_dict)\n",
    "\n",
    "    noise = OUNoise(env.action_space)\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "    max_lidar_distance = 6\n",
    "    lidar_resolution = 16\n",
    "    info_update = {'Qvals':0,\n",
    "                   'Qprime':0}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state,info = env.reset()\n",
    "        noise.reset()\n",
    "        episode_reward = 0\n",
    "        episode_cost = 0\n",
    "        episode_policy_loss = 0\n",
    "        episode_critic_loss = 0\n",
    "        \n",
    "        for step in range(num_timesteps):\n",
    "            action = agent.get_action(state)\n",
    "            action = action.squeeze()\n",
    "\n",
    "            x,y,theta_local,info_lidar = get_coords(state,max_lidar_distance,lidar_resolution)\n",
    "            theta_local = scale_angle(theta_local*180/np.pi)*np.pi/180\n",
    "            if info_lidar['within_limits']==False:\n",
    "                print(\"Episode: {} | AGENT OUTSIDE LIMITS\".format(episode))\n",
    "                obs, info = env.reset()\n",
    "                break\n",
    "\n",
    "            if np.random.uniform(0,1)>=epsilon:\n",
    "                # print('#######safe action')\n",
    "                # raise\n",
    "                filtered_action = get_safe_action(action,x,y,theta_local,csp_config)\n",
    "            else:\n",
    "                raise\n",
    "                filtered_action=action\n",
    "\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(filtered_action)\n",
    "            cost = info['cost']\n",
    "            if epsilon == 1:\n",
    "                agent.memory.push(state, action, reward-cost, new_state, terminated)\n",
    "            else:\n",
    "                agent.memory.push(state, action, reward, new_state, terminated)\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                policy_loss, critic_loss, info_update = agent.update(batch_size)       \n",
    "                episode_policy_loss += (policy_loss)\n",
    "                episode_critic_loss += (critic_loss) \n",
    "            \n",
    "            state = new_state\n",
    "            episode_reward += reward\n",
    "            episode_cost += cost\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # print(\"Episode: {} | Reward: {:.2f} | Cost : {:.2f}\".format(episode, episode_reward, episode_cost))\n",
    "\n",
    "\n",
    "                logging_dict = {'Episode':episode,\n",
    "                                'Reward': episode_reward,\n",
    "                                'Cost':episode_cost,\n",
    "                                'Actor Loss':episode_policy_loss,\n",
    "                                'Critic Loss':episode_critic_loss,\n",
    "                                'Qvals.norm':info_update['Qvals'],\n",
    "                                'Qprime.norm':info_update['Qprime']\n",
    "                                }\n",
    "                \n",
    "                if wandb_log:\n",
    "                    wandb.log(logging_dict,step=100+episode)\n",
    "                print_logging_dict(logging_dict) \n",
    "                break\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "    return agent\n",
    "    # Took 30 seconds for 10 epochs when floattensor is used\n",
    "    # Took 26 seconds for 10 epochs when floattensor was avoided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_type = 'Point'\n",
    "env_code = 1\n",
    "env_name = 'Safety{}Circle{}Gymnasium-v0'.format(agent_type,env_code)\n",
    "env = make_gymnasium_environment(env_name,render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mz9cu2lx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48224960d41465abd8a628a0d160261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Actor Loss</td><td>▁</td></tr><tr><td>Cost</td><td>▁</td></tr><tr><td>Critic Loss</td><td>▁</td></tr><tr><td>Episode</td><td>▁</td></tr><tr><td>Qprime.norm</td><td>▁</td></tr><tr><td>Qvals.norm</td><td>▁</td></tr><tr><td>Reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Actor Loss</td><td>-1963.79284</td></tr><tr><td>Cost</td><td>0.0</td></tr><tr><td>Critic Loss</td><td>10.05036</td></tr><tr><td>Episode</td><td>0</td></tr><tr><td>Qprime.norm</td><td>0.13095</td></tr><tr><td>Qvals.norm</td><td>0.13055</td></tr><tr><td>Reward</td><td>-0.36086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-cloud-93</strong> at: <a href='https://wandb.ai/crna/safety_gymnasium/runs/mz9cu2lx' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium/runs/mz9cu2lx</a><br/> View project at: <a href='https://wandb.ai/crna/safety_gymnasium' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_000422-mz9cu2lx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mz9cu2lx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cc4f5eeba94689a3a4e85e5ca364ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168198611110508, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chetan/Desktop/DDP/Code/Old/wandb/run-20240613_000513-4ix2gd43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/crna/safety_gymnasium/runs/4ix2gd43' target=\"_blank\">sleek-dew-94</a></strong> to <a href='https://wandb.ai/crna/safety_gymnasium' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/crna/safety_gymnasium' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/crna/safety_gymnasium/runs/4ix2gd43' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium/runs/4ix2gd43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0.00 | Reward:13.38 | Cost:0.00 | Actor Loss:-1956.19 | Critic Loss:8.10 | Qvals.norm:0.14 | Qprime.norm:0.14 | \n",
      "Episode:1.00 | Reward:19.76 | Cost:0.00 | Actor Loss:-1957.54 | Critic Loss:12.85 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:2.00 | Reward:3.88 | Cost:0.00 | Actor Loss:-1942.77 | Critic Loss:10.01 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:3.00 | Reward:13.03 | Cost:0.00 | Actor Loss:-1932.49 | Critic Loss:8.09 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:4.00 | Reward:-1.05 | Cost:0.00 | Actor Loss:-1925.54 | Critic Loss:8.88 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:5.00 | Reward:-1.02 | Cost:0.00 | Actor Loss:-1914.13 | Critic Loss:8.66 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:6.00 | Reward:3.86 | Cost:0.00 | Actor Loss:-1914.45 | Critic Loss:9.46 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:7.00 | Reward:2.25 | Cost:0.00 | Actor Loss:-1904.88 | Critic Loss:9.12 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:8.00 | Reward:4.03 | Cost:0.00 | Actor Loss:-1903.45 | Critic Loss:9.16 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:9.00 | Reward:6.41 | Cost:0.00 | Actor Loss:-1908.93 | Critic Loss:11.48 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:10.00 | Reward:8.59 | Cost:0.00 | Actor Loss:-1921.58 | Critic Loss:12.43 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:11.00 | Reward:-0.37 | Cost:0.00 | Actor Loss:-1938.41 | Critic Loss:12.74 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:12.00 | Reward:1.83 | Cost:0.00 | Actor Loss:-1968.68 | Critic Loss:14.58 | Qvals.norm:0.14 | Qprime.norm:0.13 | \n",
      "Episode:13.00 | Reward:-4.18 | Cost:0.00 | Actor Loss:-1996.25 | Critic Loss:15.55 | Qvals.norm:0.14 | Qprime.norm:0.14 | \n",
      "Episode:14.00 | Reward:5.83 | Cost:0.00 | Actor Loss:-2032.77 | Critic Loss:18.75 | Qvals.norm:0.13 | Qprime.norm:0.13 | \n",
      "Episode:15.00 | Reward:-3.37 | Cost:0.00 | Actor Loss:-2063.01 | Critic Loss:20.17 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:16.00 | Reward:4.62 | Cost:0.00 | Actor Loss:-2096.60 | Critic Loss:21.18 | Qvals.norm:0.14 | Qprime.norm:0.14 | \n",
      "Episode:17.00 | Reward:7.00 | Cost:0.00 | Actor Loss:-2133.77 | Critic Loss:20.38 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:18.00 | Reward:11.44 | Cost:0.00 | Actor Loss:-2159.52 | Critic Loss:18.80 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:19.00 | Reward:0.76 | Cost:0.00 | Actor Loss:-2177.04 | Critic Loss:18.61 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:20.00 | Reward:-2.18 | Cost:0.00 | Actor Loss:-2200.86 | Critic Loss:19.82 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:21.00 | Reward:-2.34 | Cost:0.00 | Actor Loss:-2236.35 | Critic Loss:22.48 | Qvals.norm:0.15 | Qprime.norm:0.15 | \n",
      "Episode:22.00 | Reward:16.25 | Cost:0.00 | Actor Loss:-2271.38 | Critic Loss:24.40 | Qvals.norm:0.16 | Qprime.norm:0.16 | \n",
      "Episode:23.00 | Reward:-2.64 | Cost:0.00 | Actor Loss:-2299.89 | Critic Loss:22.29 | Qvals.norm:0.16 | Qprime.norm:0.17 | \n",
      "Episode:24.00 | Reward:-1.30 | Cost:0.00 | Actor Loss:-2328.86 | Critic Loss:23.58 | Qvals.norm:0.16 | Qprime.norm:0.16 | \n",
      "Episode:25.00 | Reward:16.91 | Cost:0.00 | Actor Loss:-2343.39 | Critic Loss:22.58 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:26.00 | Reward:5.14 | Cost:0.00 | Actor Loss:-2362.05 | Critic Loss:20.29 | Qvals.norm:0.16 | Qprime.norm:0.16 | \n",
      "Episode:27.00 | Reward:19.49 | Cost:0.00 | Actor Loss:-2401.39 | Critic Loss:17.21 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:28.00 | Reward:20.18 | Cost:0.00 | Actor Loss:-2441.69 | Critic Loss:18.48 | Qvals.norm:0.16 | Qprime.norm:0.17 | \n",
      "Episode:29.00 | Reward:17.60 | Cost:0.00 | Actor Loss:-2486.49 | Critic Loss:18.63 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:30.00 | Reward:4.23 | Cost:0.00 | Actor Loss:-2514.84 | Critic Loss:18.03 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:31.00 | Reward:19.60 | Cost:0.00 | Actor Loss:-2544.24 | Critic Loss:17.23 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:32.00 | Reward:21.38 | Cost:0.00 | Actor Loss:-2572.47 | Critic Loss:17.21 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:33.00 | Reward:7.27 | Cost:0.00 | Actor Loss:-2594.53 | Critic Loss:17.92 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:34.00 | Reward:19.64 | Cost:0.00 | Actor Loss:-2608.26 | Critic Loss:18.67 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:35.00 | Reward:20.55 | Cost:0.00 | Actor Loss:-2621.38 | Critic Loss:19.32 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:36.00 | Reward:18.69 | Cost:0.00 | Actor Loss:-2640.30 | Critic Loss:20.32 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:37.00 | Reward:20.08 | Cost:0.00 | Actor Loss:-2708.87 | Critic Loss:22.42 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:38.00 | Reward:21.40 | Cost:0.00 | Actor Loss:-2745.47 | Critic Loss:16.82 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:39.00 | Reward:22.34 | Cost:0.00 | Actor Loss:-2792.86 | Critic Loss:17.84 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:40.00 | Reward:9.82 | Cost:0.00 | Actor Loss:-2800.00 | Critic Loss:15.57 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:41.00 | Reward:-2.43 | Cost:0.00 | Actor Loss:-2787.82 | Critic Loss:13.54 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:42.00 | Reward:16.31 | Cost:0.00 | Actor Loss:-2754.69 | Critic Loss:13.03 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:43.00 | Reward:20.21 | Cost:0.00 | Actor Loss:-2744.00 | Critic Loss:11.22 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:44.00 | Reward:5.71 | Cost:0.00 | Actor Loss:-2738.08 | Critic Loss:11.95 | Qvals.norm:0.18 | Qprime.norm:0.18 | \n",
      "Episode:45.00 | Reward:9.93 | Cost:0.00 | Actor Loss:-2734.61 | Critic Loss:10.84 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:46.00 | Reward:10.98 | Cost:0.00 | Actor Loss:-2718.12 | Critic Loss:9.82 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:47.00 | Reward:17.28 | Cost:0.00 | Actor Loss:-2693.75 | Critic Loss:8.28 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode:48.00 | Reward:5.71 | Cost:0.00 | Actor Loss:-2667.36 | Critic Loss:7.25 | Qvals.norm:0.17 | Qprime.norm:0.17 | \n",
      "Episode: 49 | AGENT OUTSIDE LIMITS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e919d175fb184b74861ddeaadc97010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Actor Loss</td><td>██████████▇▇▇▆▆▆▆▆▅▅▅▅▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▂▂▂</td></tr><tr><td>Cost</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Critic Loss</td><td>▁▃▂▁▂▂▂▂▃▃▄▄▆▇▆▆▆▆█▇█▇▅▆▆▅▅▅▆▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>Episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Qprime.norm</td><td>▂▂▂▂▂▁▁▁▁▂▂▂▂▃▃▄▅▄▅▆▅▆▇▆▆▇▇▇▇▇▇▇██▇▇▇▇▇▇</td></tr><tr><td>Qvals.norm</td><td>▂▂▂▂▂▁▁▁▁▂▂▂▂▃▃▄▄▄▅▆▅▆▇▆▆▇▇▇▇▇▇▇██▇▇▇▇▇▇</td></tr><tr><td>Reward</td><td>▆▇▃▆▂▃▃▃▄▂▃▁▄▃▄▅▂▂▆▁▂▇▇▇▇▃█▄▇█▇██▅▁▇▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Actor Loss</td><td>-2667.36048</td></tr><tr><td>Cost</td><td>0.0</td></tr><tr><td>Critic Loss</td><td>7.24569</td></tr><tr><td>Episode</td><td>48</td></tr><tr><td>Qprime.norm</td><td>0.17041</td></tr><tr><td>Qvals.norm</td><td>0.17025</td></tr><tr><td>Reward</td><td>5.70956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-dew-94</strong> at: <a href='https://wandb.ai/crna/safety_gymnasium/runs/4ix2gd43' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium/runs/4ix2gd43</a><br/> View project at: <a href='https://wandb.ai/crna/safety_gymnasium' target=\"_blank\">https://wandb.ai/crna/safety_gymnasium</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_000513-4ix2gd43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csp_config = {'truly_safe_set_bound':0.6,\n",
    "              'marginally_safe_set_bound':1.1,\n",
    "            'y_bound':2,\n",
    "            'linear_vel_angle':40, # Within 40 degrees of pointing to the obstacle, there will be a negative linear force\n",
    "            'no_turn_angle':20,\n",
    "            'parallel_angle':2} # Within 20 degrees away from the wall, there will be no turning\n",
    "\n",
    "train_config_dict = {'wandb_log':True,\n",
    "                     'num_episodes':50,\n",
    "                     'num_timesteps':500,\n",
    "                     'epsilon':0,\n",
    "                     'batch_size':512,\n",
    "}\n",
    "\n",
    "ddpg_agent_config_dict = {'hidden_size':256,\n",
    "                            'actor_learning_rate':1e-4,\n",
    "                            'critic_learning_rate':1e-3,\n",
    "                            'gamma':0.99,\n",
    "                            'tau':1e-2,\n",
    "                            'max_memory_size':50000,\n",
    "                            'weight_decay_actor':0,\n",
    "                            'weight_decay_critic':0}\n",
    "\n",
    "for _ in range(1):\n",
    "  \n",
    "    # agent = DDPGagent(env,ddpg_agent_config_dict)\n",
    "    agent = train(agent,train_config_dict,csp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent,test_config_dict,csp_config):\n",
    "\n",
    "    wandb_log = test_config_dict['wandb_log']\n",
    "    num_test_episodes = test_config_dict['num_test_episodes']\n",
    "    render = test_config_dict['render']\n",
    "    action_source = test_config_dict['agent/csp']\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.init(project='safety_gymnasium',config=test_config_dict)\n",
    "\n",
    "    agent_type = 'Point'\n",
    "    env_code = 1\n",
    "    env_name = 'Safety{}Circle{}Gymnasium-v0'.format(agent_type,env_code)\n",
    "    if render:\n",
    "        env = make_gymnasium_environment(env_name,render_mode='human')\n",
    "    else:\n",
    "        env = make_gymnasium_environment(env_name,render_mode=None)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    dummy_action = env.action_space.sample()\n",
    "    terminated, truncated = False, False\n",
    "    ep_ret, ep_cost = 0, 0\n",
    "    obs_len = env.observation_space.shape[0]\n",
    "    act_len = env.action_space.shape[0]\n",
    "    max_lidar_distance = 6\n",
    "    lidar_resolution = 16\n",
    "\n",
    "\n",
    "\n",
    "    episode = 0\n",
    "    for _ in range(num_test_episodes):\n",
    "        episode += 1\n",
    "        ep_ret, ep_cost = 0, 0\n",
    "        time_step = 0\n",
    "\n",
    "        while True:\n",
    "            time_step +=1\n",
    "            assert env.observation_space.contains(obs)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "            x,y,theta_local,info_lidar = get_coords(obs,max_lidar_distance,lidar_resolution)\n",
    "            theta_local = scale_angle(theta_local*180/np.pi)*np.pi/180\n",
    "            if info_lidar['within_limits']==False:\n",
    "                print(\"Episode: {} | AGENT OUTSIDE LIMITS\".format(episode))\n",
    "                obs, info = env.reset()\n",
    "                break\n",
    "\n",
    "            if action_source=='csp':\n",
    "                action = get_csp_action(x,y,theta_local,csp_config) \n",
    "\n",
    "            elif action_source=='agent':\n",
    "                action = agent.get_action(obs)\n",
    "                action = action.squeeze()  \n",
    "\n",
    "                # action = get_safe_action(action,x,y,theta_local,csp_config)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            # print(\"Time Step: {} | a1: {:.2f} | a2: {:.2f} | a3: {:.2f} | v1: {:.2f} | v2: {:.2f} | v3: {:.2f} | \".format(time_step,obs[0],obs[1],obs[2],obs[3],obs[4],obs[5]))\n",
    "            cost = info['cost']\n",
    "\n",
    "            ep_ret += reward\n",
    "            ep_cost += cost\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # print(\"Episode: {} | Time Step: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,time_step,ep_ret,ep_cost))\n",
    "                obs, info = env.reset()\n",
    "\n",
    "                logging_dict = {'Evaluation/Episode':episode,\n",
    "                                'Evaluation/TimeStep':time_step,\n",
    "                                'Evaluation/Reward': ep_ret,\n",
    "                                'Evaluation/Cost':ep_cost\n",
    "                                }\n",
    "                \n",
    "                if wandb_log:\n",
    "                    wandb.log(logging_dict)\n",
    "                print_logging_dict(logging_dict) \n",
    "                \n",
    "                break\n",
    "        \n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 0\n",
      "Episode: 1 | AGENT OUTSIDE LIMITS\n",
      "Episode: 2 | AGENT OUTSIDE LIMITS\n",
      "Evaluation/Episode:3.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:21.99 | Evaluation/Cost:0.00 | \n",
      "Evaluation/Episode:4.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:6.12 | Evaluation/Cost:0.00 | \n",
      "Evaluation/Episode:5.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:11.36 | Evaluation/Cost:0.00 | \n",
      "Evaluation/Episode:6.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:11.49 | Evaluation/Cost:0.00 | \n",
      "Evaluation/Episode:7.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:17.81 | Evaluation/Cost:0.00 | \n",
      "Evaluation/Episode:8.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:-3.01 | Evaluation/Cost:70.00 | \n",
      "Episode: 9 | AGENT OUTSIDE LIMITS\n",
      "Evaluation/Episode:10.00 | Evaluation/TimeStep:500.00 | Evaluation/Reward:6.57 | Evaluation/Cost:0.00 | \n"
     ]
    }
   ],
   "source": [
    "test_config_dict = {'wandb_log':False,\n",
    "                    'num_test_episodes':10,\n",
    "                    'render':True,\n",
    "                    'agent/csp':'agent'}\n",
    "\n",
    "csp_config = {'truly_safe_set_bound':0.55,\n",
    "              'marginally_safe_set_bound':1.1,\n",
    "            'y_bound':2,\n",
    "            'linear_vel_angle':40, # Within 40 degrees of pointing to the obstacle, there will be a negative linear force\n",
    "            'no_turn_angle':20,\n",
    "            'parallel_angle':10} # Within 20 degrees away from the wall, there will be no turning\n",
    "\n",
    "for i in range(1):\n",
    "  print(\"Experiment: {}\".format(i))\n",
    "  test_agent(agent=agent,test_config_dict=test_config_dict,csp_config=csp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list, y_list, theta_list = [],[],[]\n",
    "\n",
    "for experience in agent.memory.buffer:\n",
    "    state, action, reward, next_state, done = experience\n",
    "    x,y,theta_local,info_lidar = get_coords(state,max_lidar_distance,lidar_resolution)\n",
    "\n",
    "    if info_lidar['within_limits']==False:\n",
    "        print('Out of Limits')\n",
    "        continue\n",
    "\n",
    "    x_list.append(x)\n",
    "    y_list.append(y)\n",
    "    theta_list.append(theta_local)\n",
    "    \n",
    "    \n",
    "    # state_batch.append(state)\n",
    "    # action_batch.append(action)\n",
    "    # reward_batch.append(reward)\n",
    "    # next_state_batch.append(next_state)\n",
    "    # done_batch.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions, rewards, next_states, _ = agent.memory.sample(batch_size)\n",
    "np.vstack(states).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Curves for Different Linear Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "dummy_action = env.action_space.sample()\n",
    "terminated, truncated = False, False\n",
    "ep_ret, ep_cost = 0, 0\n",
    "obs_len = env.observation_space.shape[0]\n",
    "act_len = env.action_space.shape[0]\n",
    "max_lidar_distance = 6\n",
    "lidar_resolution = 16\n",
    "\n",
    "num_episodes = 1\n",
    "truly_safe_set_bound = 0.7\n",
    "y_bound = 2\n",
    "# df_row = np.zeros(len(trajectories_df.columns))\n",
    "obs_dict = {}\n",
    "act_list = [0.01,0.05,0.1,0.5,1]\n",
    "for act_linear in act_list:\n",
    "    obs_list =[]\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode += 1\n",
    "        ep_ret, ep_cost = 0, 0\n",
    "        time_step = 0\n",
    "\n",
    "        while True:\n",
    "            time_step +=1\n",
    "            assert env.observation_space.contains(obs)\n",
    "            \n",
    "            x,y,theta_local,info_lidar = get_coords(obs,max_lidar_distance,lidar_resolution)\n",
    "            theta_local = scale_angle(theta_local*180/np.pi)*np.pi/180\n",
    "            if info_lidar['within_limits']==False:\n",
    "                print(\"Episode: {} | AGENT OUTSIDE LIMITS\".format(episode))\n",
    "                obs, info = env.reset()\n",
    "                break\n",
    "\n",
    "            act = np.array([act_linear,0])\n",
    "            obs, reward, terminated, truncated, info = env.step(act)\n",
    "            obs_list.append(obs)\n",
    "            print(\"Time Step: {} | a1: {:.2f} | a2: {:.2f} | a3: {:.2f} | v1: {:.2f} | v2: {:.2f} | v3: {:.2f} | \".format(time_step,obs[0],obs[1],obs[2],obs[3],obs[4],obs[5]))\n",
    "            cost = info['cost']\n",
    "\n",
    "            ep_ret += reward\n",
    "            ep_cost += cost\n",
    "\n",
    "            if terminated or truncated:\n",
    "                print(\"Episode: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,ep_ret,ep_cost))\n",
    "                obs, info = env.reset()\n",
    "                break\n",
    "    \n",
    "    obs_dict[act_linear]=np.array(obs_list)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 0.01\n",
    "fig,ax = plt.subplots(len(act_list),6,figsize=(20,30))\n",
    "names = ['a1','a2','a3','v1','v2','v3']\n",
    "\n",
    "for i,act_linear in enumerate(act_list):\n",
    "    for j in range(6):\n",
    "        obs_name = names[j]\n",
    "        y = obs_dict[act_linear][:,j]\n",
    "        ax[i][j].plot(y)\n",
    "        ax[i][j].set_title(\"Act_{}_{}\".format(act_linear,obs_name))\n",
    "        if j==3:\n",
    "            ax[i][j].set_ylim([0,1.6])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code (Video Saving, Stochastic CSP etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frames(env,action,num_time_steps):\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    # Set seeds\n",
    "    # obs, _ = env.reset(seed=0)\n",
    "    terminated, truncated = False, False\n",
    "    ep_ret, ep_cost = 0, 0\n",
    "    video_frames = []\n",
    "    observations_lidar = []\n",
    "    for _ in range(num_time_steps):\n",
    "        assert env.observation_space.contains(obs)\n",
    "        act = env.action_space.sample()\n",
    "        act = action\n",
    "        assert env.action_space.contains(act)\n",
    "        # modified for Safe RL, added cost\n",
    "        obs, reward, cost, terminated, truncated, info = env.step(act)\n",
    "        ep_ret += reward\n",
    "        ep_cost += cost\n",
    "        # Logging\n",
    "        # video_frames.append(env.render())\n",
    "        # observations_lidar.append(obs[-16:].reshape(-1,1))\n",
    "        if terminated or truncated:\n",
    "            \n",
    "            observation, info = env.reset()\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        video_frames.append(env.render())\n",
    "\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video_frames,output_video_filename):\n",
    "    width, height,_ = video_frames[0].shape\n",
    "\n",
    "    fps = 30\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_rgb in video_frames:\n",
    "        frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_list = [-0.5]\n",
    "a2_list = [-0.5]\n",
    "\n",
    "if agent_type=='Point':\n",
    "    a1_name = 'v' # name of the first action\n",
    "    a2_name = 'w' # name of the second action\n",
    "elif agent_type=='Car':\n",
    "    a1_name = 'l' # name of the first action\n",
    "    a2_name = 'r' # name of the second action\n",
    "\n",
    "experiments = {}\n",
    "for a1 in a1_list:\n",
    "    for a2 in a2_list:\n",
    "        experiment_name = a1_name+str(a1)+'_'+a2_name+str(a2)\n",
    "        experiments[experiment_name] = np.array([a1,a2])\n",
    "\n",
    "\n",
    "print(experiments)\n",
    "\n",
    "num_time_steps = 200\n",
    "folder_name = 'Videos/Experiments_{}'.format(agent_type)\n",
    "file_prefix = 'CircleEnv_'\n",
    "\n",
    "for k,v in experiments.items():\n",
    "    video_frames = generate_frames(env,v,num_time_steps)\n",
    "    output_video_filename = folder_name+'/'+file_prefix+k+'.mp4'\n",
    "    save_video(video_frames,output_video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = safety_gymnasium.make('Safety{}Circle1-v0'.format(agent_type), render_mode='rgb_array')\n",
    "\n",
    "num_time_steps = 10\n",
    "folder_name = 'Videos/Experiments_{}'.format(agent_type)\n",
    "file_prefix = 'New_'\n",
    "v = np.array([0.1,0.5])\n",
    "k = 'v0.1_w0.5'\n",
    "\n",
    "\n",
    "video_frames = generate_frames(env,v,num_time_steps)\n",
    "output_video_filename = folder_name+'/'+file_prefix+k+'.mp4'\n",
    "save_video(video_frames,output_video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps=1000\n",
    "video_frames = generate_frames(env,v,num_time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp():\n",
    "    global ACTION\n",
    "\n",
    "    agent_type = 'Point'\n",
    "    # env = safety_gymnasium.make('Safety{}Circle1Debug-v0'.format(agent_type), render_mode='human')\n",
    "\n",
    "    env = safety_gymnasium.make(\"Safety{}Circle1-v0\".format(agent_type), render_mode='human')\n",
    "\n",
    "    # env.reset()\n",
    "    obs, info = env.reset()\n",
    "    # Set seeds\n",
    "    # obs, _ = env.reset(seed=0)\n",
    "    terminated, truncated = False, False\n",
    "    ep_ret, ep_cost = 0, 0\n",
    "    video_frames = []\n",
    "    observations_lidar = []\n",
    "    num_time_steps = 10000\n",
    "    for t in range(num_time_steps):\n",
    "        assert env.observation_space.contains(obs)\n",
    "        act = env.action_space.sample()\n",
    "        # if t<500:\n",
    "            # act = np.array([1,1])\n",
    "        # elif t>=50 and t<=100:\n",
    "        #     act = np.array([-1,-1])\n",
    "        # else:\n",
    "        #     act = np.array([0,0])\n",
    "        assert env.action_space.contains(act)\n",
    "        # print(obs[-16:].round(1))\n",
    "        # modified for Safe RL, added cost\n",
    "        print(ACTION)\n",
    "        obs, reward, cost, terminated, truncated, info = env.step(ACTION)\n",
    "        # if t%10==0:\n",
    "        # print(\"Reward: {:.2f} | Cost: {:.2f}\".format(1000*reward,cost))\n",
    "        print(\"Acc: {:.2f},{:.2f},{:.2f} | Vel: {:.2f},{:.2f},{:.2f} | Omega: {:.2f},{:.2f},{:.2f} \".format(obs[0],obs[1],obs[2],obs[3],obs[4],obs[5],obs[6],obs[7],obs[8]))\n",
    "        ep_ret += reward\n",
    "        ep_cost += cost\n",
    "        # Logging\n",
    "        # video_frames.append(env.render())\n",
    "        # observations_lidar.append(obs[-16:].reshape(-1,1))\n",
    "        if terminated or truncated:\n",
    "            print(\"### Episode Terminated ###\")\n",
    "            observation, info = env.reset()\n",
    "\n",
    "        # env.close()\n",
    "\n",
    "        video_frames.append(env.render())\n",
    "\n",
    "# def update_action():\n",
    "#     global ACTION\n",
    "#     ACTION = np.array([0,0.1])+ACTION\n",
    "#     time.sleep(0.1)\n",
    "\n",
    "ACTION = np.array([0,0])\n",
    "\n",
    "def on_press(key):\n",
    "    global obs\n",
    "    global ACTION\n",
    "    # print('{0} pressed'.format(key))\n",
    "    \n",
    "    if key == 'Key.up':\n",
    "        action = np.array([1,0])\n",
    "    elif key == Key.down:\n",
    "        action = np.array([-1,0])\n",
    "\n",
    "    elif key == Key.left:\n",
    "        action = np.array([0,1])\n",
    "\n",
    "    elif key == Key.right:\n",
    "        action = np.array([0,-1])\n",
    "\n",
    "    # elif key == Key.space:\n",
    "    #     action = np.array([0,0])\n",
    "    \n",
    "    else:\n",
    "        action = np.array([0,0])\n",
    "        print(\"Invalid Key, only arrow keys and space allowed\")\n",
    "\n",
    "    ACTION = action\n",
    "\n",
    "def on_release(key):\n",
    "    global ACTION\n",
    "    action = np.array([0,0])\n",
    "\n",
    "    # print('{0} released'.format(key))\n",
    "    ACTION = action\n",
    "\n",
    "\n",
    "def key_handling():\n",
    "    with Listener(\n",
    "        on_press=on_press,on_release=on_release) as listener:\n",
    "        listener.join()\n",
    "\n",
    "# agent_type = 'Point'\n",
    "# env = safety_gymnasium.make('Safety{}Circle1-v0'.format(agent_type), render_mode='human')\n",
    "# env.reset()\n",
    "# # env.render()\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# terminated, truncated = False, False\n",
    "# ep_ret, ep_cost = 0, 0\n",
    "# video_frames = []a\n",
    "\n",
    "# agent = Agent_Env(env)\n",
    "thread_key = Thread(target=key_handling)\n",
    "thread_key.start()\n",
    "\n",
    "thread_action = Thread(target=run_exp)\n",
    "thread_action.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput.keyboard import Key, Listener\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import safety_gymnasium\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Agent_Env:\n",
    "    def __init__(self,env):\n",
    "\n",
    "        self.ep_ret = 0\n",
    "        self.ep_cost = 0\n",
    "        self.video_frames = []\n",
    "        self.env = env\n",
    "        obs, info = env.reset()\n",
    "        self.obs = obs\n",
    "\n",
    "    def take_step(self,act):\n",
    "        global ACTION\n",
    "        act = ACTION\n",
    "        print(act)\n",
    "\n",
    "        # assert self.env.observation_space.contains(obs)\n",
    "        assert self.env.action_space.contains(act)\n",
    "        self.obs, reward, cost, terminated, truncated, info = self.env.step(act)\n",
    "        print('here0')\n",
    "        self.ep_ret += reward\n",
    "        self.ep_cost += cost\n",
    "        print('here1')\n",
    "        if terminated or truncated:\n",
    "            \n",
    "            self.obs, info = self.env.reset()\n",
    "\n",
    "        self.self.env.close()\n",
    "        print('herer')\n",
    "        self.video_frames.append(self.env.render())\n",
    "\n",
    "    def get_state(self):\n",
    "        print(self.obs[0])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "obs, info = env.reset()\n",
    "terminated, truncated = False, False\n",
    "ep_ret, ep_cost = 0, 0\n",
    "video_frames = []\n",
    "\n",
    "ACTION = None\n",
    "def on_press(key):\n",
    "    global obs\n",
    "    print('{0} pressed'.format(key))\n",
    "    \n",
    "    if key == Key.up:\n",
    "        action = np.array([1,0])\n",
    "    elif key == Key.down:\n",
    "        action = np.array([-1,0])\n",
    "\n",
    "    elif key == Key.left:\n",
    "        action = np.array([0,1])\n",
    "\n",
    "    elif key == Key.right:\n",
    "        action = np.array([0,-1])\n",
    "\n",
    "    # elif key == Key.space:\n",
    "    #     action = np.array([0,0])\n",
    "    \n",
    "    else:\n",
    "        action = np.array([0,0])\n",
    "        print(\"Invalid Key, only arrow keys and space allowed\")\n",
    "\n",
    "    ACTION = action\n",
    "    # # agent.take_step(action)\n",
    "    # assert env.observation_space.contains(obs)\n",
    "    # act = env.action_space.sample()\n",
    "    # act = action\n",
    "    # assert env.action_space.contains(act)\n",
    "    # # modified for Safe RL, added cost\n",
    "    # obs, reward, cost, terminated, truncated, info = env.step(act)\n",
    "    # ep_ret += reward\n",
    "    # ep_cost += cost\n",
    "    # # Logging\n",
    "    # # video_frames.append(env.render())\n",
    "    # # observations_lidar.append(obs[-16:].reshape(-1,1))\n",
    "    # if terminated or truncated:\n",
    "        \n",
    "    #     observation, info = env.reset()\n",
    "\n",
    "    # # env.close()\n",
    "\n",
    "    # video_frames.append(env.render())\n",
    "\n",
    "def on_release(key):\n",
    "    action = np.array([0,0])\n",
    "\n",
    "    print('{0} released'.format(key))\n",
    "    ACTION = action\n",
    "\n",
    "\n",
    "def key_handling():\n",
    "    with Listener(\n",
    "        on_press=on_press) as listener:\n",
    "        listener.join()\n",
    "\n",
    "def run_exp():\n",
    "    print(ACTION)\n",
    "\n",
    "# env = safety_gymnasium.make('SafetyPointCircle1-v0', render_mode='rgb_array')\n",
    "agent_type = 'Point'\n",
    "env = safety_gymnasium.make('Safety{}Circle1-v0'.format(agent_type), render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# terminated, truncated = False, False\n",
    "# ep_ret, ep_cost = 0, 0\n",
    "# video_frames = []\n",
    "\n",
    "agent = Agent_Env(env)\n",
    "thread_key = Thread(target=key_handling)\n",
    "thread_key.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput.keyboard import Key, Listener\n",
    "from threading import Thread\n",
    "\n",
    "def on_press(key):\n",
    "    print(1)\n",
    "def key_handling():\n",
    "\n",
    "    with Listener(\n",
    "        on_press=on_press) as listener:\n",
    "        listener.join()\n",
    "\n",
    "thread_key = Thread(target=key_handling)\n",
    "thread_key.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "\n",
    "while True:\n",
    "    keyboard.on_press_key(\"P\", lambda _:print(\"You pressed p\"))\n",
    "    \n",
    "\n",
    "keyboard.unhook_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import beta as beta_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_pdf(x, alpha, beta):\n",
    "    # Compute the PDF of beta distribution at x\n",
    "    pdf = (x**(alpha - 1) * (1 - x)**(beta - 1)) / beta_function(alpha, beta)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "action_range = [0,1]\n",
    "resolution = 1000\n",
    "x_csp_det = 0.7\n",
    "\n",
    "x_axis = np.linspace(action_range[0],action_range[1],resolution)\n",
    "y_axis_det = np.zeros(resolution)\n",
    "x_idx = int(resolution*(x_csp_det-action_range[0])/(action_range[1]-action_range[0]))\n",
    "x_range = action_range[1]-action_range[0]\n",
    "y_axis_det[x_idx] = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters of the beta distribution\n",
    "alpha = 8.0  # shape parameter alpha (> 0)\n",
    "beta = 5.0   # shape parameter beta (> 0)\n",
    "\n",
    "# Generate x values for PDF plot\n",
    "x = np.linspace(0, 1, resolution)\n",
    "\n",
    "# Calculate the PDF values\n",
    "y_axis_beta = beta_pdf(x, alpha, beta)\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].plot(x_axis,y_axis_det)\n",
    "ax[0].set_xlim([-0.2,1.2])\n",
    "ax[0].set_ylim([0,4])\n",
    "\n",
    "ax[1].plot(x_axis,y_axis_beta)\n",
    "ax[1].set_xlim([-0.2,1.2])\n",
    "ax[1].set_ylim([0,4])\n",
    "\n",
    "ax[0].set_title('Deterministic CSP')\n",
    "ax[1].set_title('Stochastic CSP')\n",
    "ax[0].set_xlabel('Action Range')\n",
    "ax[1].set_xlabel('Action Range')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "# Set seeds\n",
    "# obs, _ = env.reset(seed=0)\n",
    "\n",
    "dummy_action = env.action_space.sample()\n",
    "terminated, truncated = False, False\n",
    "ep_ret, ep_cost = 0, 0\n",
    "# video_frames = []\n",
    "# observations_lidar = []\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "        \n",
    "    while True:\n",
    "        assert env.observation_space.contains(obs)\n",
    "        # act = env.action_space.sample()\n",
    "        # if t<500:\n",
    "            # act = np.array([1,1])\n",
    "        # elif t>=50 and t<=100:\n",
    "        #     act = np.array([-1,-1])\n",
    "        # else:\n",
    "        #     act = np.array([0,0])\n",
    "        # assert env.action_space.contains(act)\n",
    "        # print(obs[-16:].round(1))\n",
    "        # modified for Safe RL, added cost\n",
    "        # print(act)\n",
    "        obs, reward, cost, terminated, truncated, info = env.step(dummy_action)\n",
    "        lidar_values = obs[-16:]\n",
    "        # print(np.round(lidar_values,1))\n",
    "        # print(info['action_keyboard'])\n",
    "        # if t%10==0:\n",
    "        # print(\"Reward: {:.2f} | Cost: {:.2f}\".format(1000*reward,cost))\n",
    "        # print(\"Acc: {:.2f},{:.2f},{:.2f} | Vel: {:.2f},{:.2f},{:.2f} | Omega: {:.2f},{:.2f},{:.2f} \".format(obs[0],obs[1],obs[2],obs[3],obs[4],obs[5],obs[6],obs[7],obs[8]))\n",
    "        ep_ret += reward\n",
    "        ep_cost += cost\n",
    "        # Logging\n",
    "        # video_frames.append(env.render())\n",
    "        # observations_lidar.append(obs[-16:].reshape(-1,1))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode: {} | Reward: {:.2f} | Cost: {:.2f}\".format(episode,ep_ret,ep_cost))\n",
    "            observation, info = env.reset()\n",
    "            break\n",
    "\n",
    "    # env.close()\n",
    "\n",
    "    # video_frame = env.render()\n",
    "    # cv2.imshow(\"Frame\",video_frame)\n",
    "    # key = cv2.waitKey(10)\n",
    "    # # print(key)\n",
    "    # if key == ord('q'):\n",
    "    #     print(\"### Episode Terminated ###\")\n",
    "    #     break\n",
    "    # video_frames.append(env.render())\n",
    "# env.close()\n",
    "# cv2.destroyAllWindows()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
